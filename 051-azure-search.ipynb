{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://learn.microsoft.com/en-us/azure/ai-studio/tutorials/copilot-sdk-create-resources?tabs=windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy\n",
    "%pip install azure-ai-projects azure-ai-inference azure-identity \n",
    "%pip install azure-search-documents\n",
    "%pip install azure-ai-evaluation  azure_ai-evaluation[prompts]\n",
    "%pip install azure-monitor-opentelemetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import logging\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "\n",
    "# load environment variables from the .env file\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set \"./assets\" as the path where assets are stored, resolving the absolute path:\n",
    "# ASSET_PATH = pathlib.Path(\"../..\").parent.resolve() / \"data\"\n",
    "ASSET_PATH = pathlib.Path(\"..\").resolve() / \"data\"\n",
    "ASSET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an root app logger that prints info level logs to stdout\n",
    "logger = logging.getLogger(\"app\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Returns a module-specific logger, inheriting from the root app logger\n",
    "def get_logger(module_name):\n",
    "    return logging.getLogger(f\"app.{module_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Enable instrumentation and logging of telemetry to the project\n",
    "def enable_telemetry(log_to_project: bool = False):\n",
    "    AIInferenceInstrumentor().instrument()\n",
    "\n",
    "    # enable logging message contents\n",
    "    os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "\n",
    "    if log_to_project:\n",
    "        from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "\n",
    "        project = AIProjectClient.from_connection_string(\n",
    "            conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    "        )\n",
    "        tracing_link = f\"https://ai.azure.com/tracing?wsid=/subscriptions/{project.scope['subscription_id']}/resourceGroups/{project.scope['resource_group_name']}/providers/Microsoft.MachineLearningServices/workspaces/{project.scope['project_name']}\"\n",
    "        application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "        if not application_insights_connection_string:\n",
    "            logger.warning(\n",
    "                \"No application insights configured, telemetry will not be logged to project. Add application insights at:\"\n",
    "            )\n",
    "            logger.warning(tracing_link)\n",
    "\n",
    "            return\n",
    "\n",
    "        configure_azure_monitor(connection_string=application_insights_connection_string)\n",
    "        logger.info(\"Enabled telemetry logging to project, view traces at:\")\n",
    "        logger.info(tracing_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a project client using environment variables loaded from the .env file\n",
    "project = AIProjectClient.from_connection_string(\n",
    "    conn_str=os.environ[\"AIPROJECT_CONNECTION_STRING\"], credential=DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "chat = project.inference.get_chat_completions_client()\n",
    "\n",
    "# create a vector embeddings client that will be used to generate vector embeddings\n",
    "embeddings = project.inference.get_embeddings_client()\n",
    "\n",
    "# use the project client to get the default search connection\n",
    "search_connection = project.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_AI_SEARCH, include_credentials=True\n",
    ")\n",
    "\n",
    "print (\"default search \", search_connection.endpoint_url)\n",
    "\n",
    "# Create a search index client using the search connection\n",
    "# This client will be used to create and delete search indexes\n",
    "# index_client = SearchIndexClient(\n",
    "#     endpoint=search_connection.endpoint_url, credential=AzureKeyCredential(key=search_connection.key)\n",
    "# )\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"], credential=AzureKeyCredential(key=os.environ[\"AZURE_SEARCH_ADMIN_KEY\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AzureKeyCredential(key=search_connection.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SemanticSearch,\n",
    "    SearchField,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SearchFieldDataType,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    HnswParameters,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    ")\n",
    "\n",
    "\n",
    "def create_index_definition(index_name: str, model: str) -> SearchIndex:\n",
    "    dimensions = 1536  # text-embedding-ada-002\n",
    "    if model == \"text-embedding-3-large\":\n",
    "        dimensions = 3072\n",
    "\n",
    "    # The fields we want to index. The \"embedding\" field is a vector field that will\n",
    "    # be used for vector search.\n",
    "    fields = [\n",
    "        SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "        SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"filepath\", type=SearchFieldDataType.String),\n",
    "        SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "        SimpleField(name=\"url\", type=SearchFieldDataType.String),\n",
    "        SearchField(\n",
    "            name=\"contentVector\",\n",
    "            type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "            searchable=True,\n",
    "            # Size of the vector created by the text-embedding-ada-002 model.\n",
    "            vector_search_dimensions=dimensions,\n",
    "            vector_search_profile_name=\"myHnswProfile\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # The \"content\" field should be prioritized for semantic ranking.\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"default\",\n",
    "        prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"title\"),\n",
    "            keywords_fields=[],\n",
    "            content_fields=[SemanticField(field_name=\"content\")],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # For vector search, we want to use the HNSW (Hierarchical Navigable Small World)\n",
    "    # algorithm (a type of approximate nearest neighbor search algorithm) with cosine\n",
    "    # distance.\n",
    "    vector_search = VectorSearch(\n",
    "        algorithms=[\n",
    "            HnswAlgorithmConfiguration(\n",
    "                name=\"myHnsw\",\n",
    "                kind=VectorSearchAlgorithmKind.HNSW,\n",
    "                parameters=HnswParameters(\n",
    "                    m=4,\n",
    "                    ef_construction=1000,\n",
    "                    ef_search=1000,\n",
    "                    metric=VectorSearchAlgorithmMetric.COSINE,\n",
    "                ),\n",
    "            ),\n",
    "            ExhaustiveKnnAlgorithmConfiguration(\n",
    "                name=\"myExhaustiveKnn\",\n",
    "                kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "                parameters=ExhaustiveKnnParameters(metric=VectorSearchAlgorithmMetric.COSINE),\n",
    "            ),\n",
    "        ],\n",
    "        profiles=[\n",
    "            VectorSearchProfile(\n",
    "                name=\"myHnswProfile\",\n",
    "                algorithm_configuration_name=\"myHnsw\",\n",
    "            ),\n",
    "            VectorSearchProfile(\n",
    "                name=\"myExhaustiveKnnProfile\",\n",
    "                algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index definition\n",
    "    return SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "        semantic_search=semantic_search,\n",
    "        vector_search=vector_search,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for indexing a csv file, that adds each row as a document\n",
    "# and generates vector embeddings for the specified content_column\n",
    "def create_docs_from_csv(path: str, content_column: str, model: str) -> list[dict[str, any]]:\n",
    "    products = pd.read_csv(path)\n",
    "    items = []\n",
    "    for product in products.to_dict(\"records\"):\n",
    "        content = product[content_column]\n",
    "        id = str(product[\"id\"])\n",
    "        title = product[\"name\"]\n",
    "        url = f\"/products/{title.lower().replace(' ', '-')}\"\n",
    "        emb = embeddings.embed(input=content, model=model)\n",
    "        rec = {\n",
    "            \"id\": id,\n",
    "            \"content\": content,\n",
    "            \"filepath\": f\"{title.lower().replace(' ', '-')}\",\n",
    "            \"title\": title,\n",
    "            \"url\": url,\n",
    "            \"contentVector\": emb.data[0].embedding,\n",
    "        }\n",
    "        items.append(rec)\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def create_index_from_csv(index_name, csv_file):\n",
    "    # If a search index already exists, delete it:\n",
    "    try:\n",
    "        index_definition = index_client.get_index(index_name)\n",
    "        index_client.delete_index(index_name)\n",
    "        logger.info(f\"🗑️  Found existing index named '{index_name}', and deleted it\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # create an empty search index\n",
    "    print (\"creating index\")\n",
    "    index_definition = create_index_definition(index_name, model=os.environ[\"EMBEDDINGS_MODEL\"])\n",
    "    index_client.create_index(index_definition)\n",
    "\n",
    "    print (\"create records embedding\")\n",
    "    # create documents from the products.csv file, generating vector embeddings for the \"description\" column\n",
    "    docs = create_docs_from_csv(path=csv_file, content_column=\"description\", model=os.environ[\"EMBEDDINGS_MODEL\"])\n",
    "\n",
    "    print (\"adding records to index\")\n",
    "    # Add the documents to the index using the Azure AI Search client\n",
    "\n",
    "    \n",
    "\n",
    "    search_client = SearchClient(\n",
    "        endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"],\n",
    "        index_name=index_name,\n",
    "        credential=AzureKeyCredential(key=os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]),\n",
    "    )\n",
    "\n",
    "    search_client.upload_documents(docs)\n",
    "    logger.info(f\"➕ Uploaded {len(docs)} documents to '{index_name}' index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSET_PATH.joinpath(\"products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_index_from_csv(os.environ[\"AISEARCH_INDEX_NAME\"], str(ASSET_PATH.joinpath(\"products.csv\")) #str(ASSET_PATH.joinpath(\"products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now use AI Search capablities via code\n",
    "from azure.ai.inference.prompts import PromptTemplate\n",
    "from azure.search.documents.models import VectorizedQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentelemetry import trace\n",
    "from pathlib import Path\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    " \n",
    "# Create a search index client using the search connection\n",
    "# This client will be used to create and delete search indexes\n",
    "search_client = SearchClient(\n",
    "    index_name=os.environ[\"AISEARCH_INDEX_NAME\"],\n",
    "    endpoint=os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"],\n",
    "    credential=AzureKeyCredential(key=os.environ[\"AZURE_SEARCH_ADMIN_KEY\"]),\n",
    ")\n",
    "\n",
    "@tracer.start_as_current_span(name=\"get_product_documents\")\n",
    "def get_product_documents(messages: list, context: dict = None) -> dict:\n",
    "    if context is None:\n",
    "        context = {}\n",
    "\n",
    "    overrides = context.get(\"overrides\", {})\n",
    "    top = overrides.get(\"top\", 5)\n",
    "\n",
    "    # generate a search query from the chat messages\n",
    "    intent_prompty = PromptTemplate.from_prompty(Path(ASSET_PATH) / \"prompts\" / \"intent_mapping.prompty\")\n",
    "\n",
    "    intent_mapping_response = chat.complete(\n",
    "        model=os.environ[\"INTENT_MAPPING_MODEL\"],\n",
    "        messages=intent_prompty.create_messages(conversation=messages),\n",
    "        **intent_prompty.parameters,\n",
    "    )\n",
    "\n",
    "    search_query = intent_mapping_response.choices[0].message.content\n",
    "    logger.debug(f\"🧠 Intent mapping: {search_query}\")\n",
    "\n",
    "    # generate a vector representation of the search query\n",
    "    embedding = embeddings.embed(model=os.environ[\"EMBEDDINGS_MODEL\"], input=search_query)\n",
    "    search_vector = embedding.data[0].embedding\n",
    "\n",
    "    # search the index for products matching the search query\n",
    "    vector_query = VectorizedQuery(vector=search_vector, k_nearest_neighbors=top, fields=\"contentVector\")\n",
    "\n",
    "    search_results = search_client.search(\n",
    "        search_text=search_query, vector_queries=[vector_query], select=[\"id\", \"content\", \"filepath\", \"title\", \"url\"]\n",
    "    )\n",
    "\n",
    "    documents = [\n",
    "        {\n",
    "            \"id\": result[\"id\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"filepath\": result[\"filepath\"],\n",
    "            \"title\": result[\"title\"],\n",
    "            \"url\": result[\"url\"],\n",
    "        }\n",
    "        for result in search_results\n",
    "    ]\n",
    "\n",
    "    # add results to the provided context\n",
    "    if \"thoughts\" not in context:\n",
    "        context[\"thoughts\"] = []\n",
    "\n",
    "    # add thoughts and documents to the context object so it can be returned to the caller\n",
    "    context[\"thoughts\"].append(\n",
    "        {\n",
    "            \"title\": \"Generated search query\",\n",
    "            \"description\": search_query,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if \"grounding_data\" not in context:\n",
    "        context[\"grounding_data\"] = []\n",
    "    context[\"grounding_data\"].append(documents)\n",
    "\n",
    "    logger.debug(f\"📄 {len(documents)} documents retrieved: {documents}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I need a new tent for 4 people, what would you recommend?\"\n",
    "result = get_product_documents(messages=[{\"role\": \"user\", \"content\": query}])\n",
    "\n",
    "import json\n",
    "print (json.dumps(result, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_telemetry(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat app with RAG Capablities. So far, we have interfaced search client. now with chat app\n",
    "\n",
    "from azure.ai.inference.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(name=\"chat_with_products\")\n",
    "def chat_with_products(messages: list, context: dict = None) -> dict:\n",
    "    if context is None:\n",
    "        context = {}\n",
    "\n",
    "    documents = get_product_documents(messages, context)\n",
    "\n",
    "    # do a grounded chat call using the search results\n",
    "    grounded_chat_prompt = PromptTemplate.from_prompty(Path(ASSET_PATH) / \"prompts\" / \"grounded_chat.prompty\")\n",
    "\n",
    "    system_message = grounded_chat_prompt.create_messages(documents=documents, context=context)\n",
    "    response = chat.complete(\n",
    "        model=os.environ[\"CHAT_MODEL\"],\n",
    "        messages=system_message + messages,\n",
    "        **grounded_chat_prompt.parameters,\n",
    "    )\n",
    "    logger.info(f\"💬 Response: {response.choices[0].message}\")\n",
    "\n",
    "    # Return a chat protocol compliant response\n",
    "    return {\"message\": response.choices[0].message, \"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run chat with products\n",
    "response = chat_with_products(messages=[{\"role\": \"user\", \"content\": \"I need a new tent for 4 people, what would you recommend?\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation dataset, checking the quality of the chat response, then deploy to store\n",
    "# data/evals/chat_eval_data.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate, GroundednessEvaluator\n",
    "\n",
    "\n",
    "evaluator_model = {\n",
    "    \"azure_endpoint\": connection.endpoint_url,\n",
    "    \"azure_deployment\": os.environ[\"EVALUATION_MODEL\"],\n",
    "    \"api_version\": \"2024-06-01\",\n",
    "    \"api_key\": connection.key,\n",
    "}\n",
    "\n",
    "groundedness = GroundednessEvaluator(evaluator_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chat_with_products(query):\n",
    "    response = chat_with_products(messages=[{\"role\": \"user\", \"content\": query}])\n",
    "    return {\"response\": response[\"message\"].content, \"context\": response[\"context\"][\"grounding_data\"]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workaround for multiprocessing issue on linux\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import contextlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with contextlib.suppress(RuntimeError):\n",
    "    multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run evaluation with a dataset and target function, log to the project\n",
    "result = evaluate(\n",
    "    data=Path(ASSET_PATH) / \"evals\" / \"chat_eval_data.jsonl\",\n",
    "    target=evaluate_chat_with_products,\n",
    "    evaluation_name=\"evaluate_chat_with_products\",\n",
    "    evaluators={\n",
    "        \"groundedness\": groundedness,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"query\": {\"${data.query}\"},\n",
    "            \"response\": {\"${target.response}\"},\n",
    "            \"context\": {\"${target.context}\"},\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=project.scope,\n",
    "    output_path= Path(ASSET_PATH) / \"..\" / \"output\" / \"./myevalresults.json\",\n",
    ")\n",
    "\n",
    "tabular_result = pd.DataFrame(result.get(\"rows\"))\n",
    "\n",
    "pprint(\"-----Summarized Metrics-----\")\n",
    "pprint(result[\"metrics\"])\n",
    "pprint(\"-----Tabular Result-----\")\n",
    "pprint(tabular_result)\n",
    "pprint(f\"View evaluation results in AI Studio: {result['studio_url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
